{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIy9DQtZRffs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the data in as usual \n",
        "!pip install tf-nightly --quiet\n",
        "!pip install python-resize-image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "import urllib.request\n",
        "import os.path\n",
        "from typing import Optional, List, Callable\n",
        "from PIL import Image\n",
        "import shutil, os\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, List, Callable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil, os\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import Counter\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # ignore warnings\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import model_from_json\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4WJ84Fd0A79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount= True)\n",
        "# need to mount to Google Drive\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/covid-chestxray-dataset-master/COVID19_images\") ### for my testing\n",
        "!pwd # should see /content/gdrive/My Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqCjor4pzfuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_k_splits(train_dsu):\n",
        "  all_images = []\n",
        "  all_labels = []\n",
        "  for image, label in train_dsu.take(-1):\n",
        "    all_images.append(image.numpy())\n",
        "    all_labels.append(int(label))\n",
        "  all_images = np.array(all_images)\n",
        "  all_labels = np.array(all_labels) \n",
        "  print(\"done\")\n",
        "  return (all_images, all_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0pX7weozf1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_augmentation = keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Oyp7cAzf7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup \n",
        "image_size = (160, 160) #### \n",
        "image_shape = image_size + (3,)\n",
        "batch_size = 32 \n",
        "\n",
        "# training set \n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory('images_directory',\n",
        "    validation_split=0.25, subset='training', seed=1337,\n",
        "    image_size=image_size, batch_size=batch_size)\n",
        "\n",
        "# test set - 25% of the data\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "   'images_directory', validation_split=0.25, subset='validation', seed=1337,\n",
        "    image_size=image_size, batch_size=batch_size)\n",
        "\n",
        "train_dsu = tf.data.Dataset.unbatch(train_ds) # <_UnbatchDataset shapes: ((160, 160, 3), ()), types: (tf.float32, tf.int32)>\n",
        "del train_ds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9pJ5Jiz0-T9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Batching:\n",
        "def batch_generator(X, Y, batch_size):\n",
        "    indices = np.arange(len(X)) \n",
        "    batch=[]\n",
        "    while True:\n",
        "            # shuffle your data before each epoch\n",
        "            np.random.shuffle(indices) \n",
        "            for i in indices:\n",
        "                batch.append(i)\n",
        "                if len(batch)==batch_size:\n",
        "                    yield X[batch], Y[batch]\n",
        "                    batch=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPl1bHNB1Qgi",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--dEStef1Dnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train \n",
        "x = get_k_splits(train_dsu) #, 2) # just out 2 by default here. doesn't matter\n",
        "print (\"got x\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WcdBEk41PeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrained model with SMOTE \n",
        "modelname = \"P6SMOTEfinal\" ##### MODEL NAME\n",
        "# K-fold Cross Validation model evaluation\n",
        "\n",
        "num_folds = 3 # THREE FOLDS\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "print (\"got kfold\")\n",
        "input_shape = image_shape\n",
        "no_classes = 3\n",
        "\n",
        "fold_no = 0\n",
        "for train, val in kfold.split(x[0], x[1]):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')  \n",
        "  unique, counts = np.unique(x[1][train], return_counts=True)\n",
        "  print(\"COUNTS: \", dict(zip(unique, counts)))\n",
        "\n",
        "  # SMOTE\n",
        "  sm = SMOTE(random_state=42)\n",
        "  hold_shape = x[0][train].shape\n",
        "  X_train = x[0][train].reshape(hold_shape[0],hold_shape[1]*hold_shape[2]*hold_shape[3])\n",
        "  y_train = x[1][train].reshape(hold_shape[0],1)\n",
        "  X_smote, y_smote = sm.fit_resample(X_train, y_train)\n",
        "  new_shape = X_smote.shape[0]\n",
        "  a = X_smote.reshape(new_shape,hold_shape[1],hold_shape[2],hold_shape[3])\n",
        "  b = y_smote.reshape(new_shape,)\n",
        "  X_smote = a\n",
        "  y_smote = b\n",
        "  del a\n",
        "  del b\n",
        "  del X_train\n",
        "  del y_train\n",
        "  \n",
        "  print(\"SMOTE X SHAPE:\", X_smote.shape)\n",
        "  print(\"SMOTE Y SHAPE:\", X_smote.shape)\n",
        "\n",
        "  unique, counts = np.unique(y_smote, return_counts=True)\n",
        "  print(\"SMOTE COUNTS: \", dict(zip(unique, counts)))\n",
        "  \n",
        "  # batch it up right before running \n",
        "  train_generator = batch_generator(X_smote, y_smote, 32)\n",
        "  del X_smote\n",
        "  del y_smote\n",
        "\n",
        "  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "  prediction_layer = tf.keras.layers.Dense(3, activation='softmax')\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=image_shape,\n",
        "                                                include_top=False,\n",
        "                                                weights='imagenet')\n",
        "\n",
        "  model = tf.keras.Sequential([layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "                               layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "                               base_model,\n",
        "                               tf.keras.layers.GlobalAveragePooling2D(),\n",
        "                               tf.keras.layers.Dense(3, activation='softmax')])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), # smaller learning rate \n",
        "                loss= \"SparseCategoricalCrossentropy\", #weighted_ce , \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_generator, #X_smote, y_smote,\n",
        "                      batch_size=batch_size,\n",
        "                      validation_data=(x[0][val], x[1][val]),\n",
        "                      epochs=100,\n",
        "                      steps_per_epoch = 100,\n",
        "                      class_weight = {0: 4., 1: 26., 2: 16.}) \n",
        "\n",
        "  model_json = model.to_json()\n",
        "  with open(\"model_\" + modelname + time.strftime(\"%Y%m%d-%H%M%S\") + \".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "  model.save_weights(\"model_\" + modelname + time.strftime(\"%Y%m%d-%H%M%S\") + \".h5\")\n",
        "  print(time.strftime(\"%Y%m%d-%H%M%S\") + \"Saved model to disk \\n\")\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(x[0][val], x[1][val], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "  #### get the classification report \n",
        "  predictions = model.predict(x[0][val])\n",
        "  print(predictions)\n",
        "  labels = np.argmax(predictions, axis=-1)    \n",
        "  print(labels)\n",
        "  target_names = ['class 0', 'class 1', 'class 2']\n",
        "  print(classification_report(y_true=x[1][val], y_pred=labels, target_names=target_names)) #, output_dict=True))\n",
        "  my_eval = classification_report(y_true=x[1][val], y_pred=labels, target_names=target_names,output_dict=True)\n",
        "  my_eval_df= pd.DataFrame(my_eval).transpose()\n",
        "  pairs = [(i, j) for i,j in zip(labels, x[1][val])]\n",
        "  print(Counter(elem for elem in pairs))\n",
        "  temp = dict(Counter(elem for elem in pairs))\n",
        "\n",
        "  for j in range(3):\n",
        "    name = \"pred_\" + str(j)\n",
        "    my_eval_df[name]=[0,0,0,\"-\",\"-\",\"-\"]\n",
        "    for i in range(3):\n",
        "      if (j,i) in temp.keys():\n",
        "        my_eval_df[name][i] = temp[(j,i)]\n",
        "  print(my_eval_df)\n",
        "  my_eval_df.to_csv(modelname + '_classification_report_' + str(fold_no) + '.csv', index=True)\n",
        "  my_eval_df = 0 # save space \n",
        "  del my_eval\n",
        "  del my_eval_df\n",
        "  del temp \n",
        "\n",
        "  print(history.history.keys())\n",
        "#  \"Accuracy\"\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.savefig(modelname + \"_accuracy_\" + str(fold_no) + \".png\")\n",
        "  plt.show()\n",
        "  \n",
        "  # \"Loss\"\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.savefig(modelname + \"_loss_\" + str(fold_no) + \".png\")\n",
        "  plt.show()\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "\n",
        "# save results \n",
        "temp = pd.DataFrame(list(zip(acc_per_fold, loss_per_fold)), \n",
        "               columns =['Accuracy', 'Loss'])\n",
        "#temp = temp.T\n",
        "temp.loc['mean'] = temp.mean()\n",
        "print(temp)\n",
        "temp.to_csv(modelname + '.csv', index=True)\n",
        "del temp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYH-iv3-1kWb",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2OUpjZO1j18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dsu = tf.data.Dataset.unbatch(test_ds) # <_UnbatchDataset shapes: ((160, 160, 3), ()), types: (tf.float32, tf.int32)>\n",
        "y = get_k_splits(test_dsu)\n",
        "model.evaluate(y[0],y[1], verbose=0)   \n",
        "# test data \n",
        "test_predictions = model.predict(y[0])\n",
        "test_labels = np.argmax(test_predictions, axis=-1)    \n",
        "# print(labels)\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "print(classification_report(y_true=y[1], y_pred=test_labels, target_names=target_names)) #, output_dict=True))\n",
        "test_my_eval = classification_report(y_true=y[1], y_pred=test_labels, target_names=target_names,output_dict=True)\n",
        "test_my_eval_df= pd.DataFrame(test_my_eval).transpose()\n",
        "test_pairs = [(i, j) for i,j in zip(test_labels, y[1])]\n",
        "# print(Counter(elem for elem in test_pairs))\n",
        "test_temp = dict(Counter(elem for elem in test_pairs))\n",
        "for j in range(3):\n",
        "  name = \"pred_\" + str(j)\n",
        "  test_my_eval_df[name]=[0,0,0,\"-\",\"-\",\"-\"]\n",
        "  for i in range(3):\n",
        "    if (j,i) in test_temp.keys():\n",
        "      test_my_eval_df[name][i] = test_temp[(j,i)]\n",
        "print(test_my_eval_df)\n",
        "test_my_eval_df.to_csv(modelname + '_test_classification_report_' + str(fold_no) + '.csv', index=True)\n",
        "test_my_eval_df = 0 # save space "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}